{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#use a fixed seed for reproducibility\n",
    "#seed = np.random.randint(10000, size=1)[0]\n",
    "#print(seed)\n",
    "seed = 6016\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Samples: 59800\n",
      "Features: 869\n",
      "Histogram: check all 0s and 1s, no -1s etc.\n",
      "(array([57380,     0,     0,     0,     0,     0,     0,     0,     0,  2420]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n",
      "Training set\n",
      "(35880, 869)\n",
      "(array([34431,     0,     0,     0,     0,     0,     0,     0,     0,  1449]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n",
      "Xval set\n",
      "(11960, 869)\n",
      "(array([11451,     0,     0,     0,     0,     0,     0,     0,     0,   509]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n",
      "Test set\n",
      "(11960, 869)\n",
      "(array([11498,     0,     0,     0,     0,     0,     0,     0,     0,   462]),\n",
      " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# 1. load data\n",
    "############################################################\n",
    "\n",
    "# load dataset\n",
    "print(\"Loading data...\")\n",
    "dataframe = pd.read_csv(\"dump.txt\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "num_samples, num_features = dataset.shape\n",
    "num_features -=1\n",
    "num_labels=1\n",
    "print(\"Samples: %d\\nFeatures: %d\" % (num_samples, num_features))\n",
    "\n",
    "y=dataset[:,num_features].astype(float)\n",
    "print(\"Histogram: check all 0s and 1s, no -1s etc.\")\n",
    "pprint(np.histogram(y))\n",
    "\n",
    "X = dataset[:,0:num_features].astype(float)\n",
    "# split into training, xval, test, 60/20/20\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_xval, X_test, y_xval, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "print \"Training set\"\n",
    "print X_train.shape\n",
    "pprint(np.histogram(y_train))\n",
    "\n",
    "print \"Xval set\"\n",
    "print X_xval.shape\n",
    "pprint(np.histogram(y_xval))\n",
    "\n",
    "print \"Test set\"\n",
    "print X_test.shape\n",
    "pprint(np.histogram(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe3b0841a10>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIScgl5EJOT+H0FU9hjQKvapAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3e76f2a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scree chart to see how much variation is explained by how many predictors\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "pca = PCA(n_components=869)\n",
    "pca.fit(X_train)\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "#print(var1)\n",
    "%matplotlib inline\n",
    "plt.plot(var1)\n",
    "\n",
    "# looks like ~100 orthogonal PCA components explain > 50% of the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a custom f-score metric\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # return keras tensor for recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # return keras tensor for precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def f_score(y_true, y_pred):\n",
    "    beta = 2 # penalize false positives\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    "    #my_precision = precision(y_true, y_pred)\n",
    "    #my_recall = recall(y_true, y_pred)\n",
    "    \n",
    "    #f_score = (1 +( beta **2)) * my_precision * my_recall / ((beta ** 2) * my_precision + my_recall)\n",
    "\n",
    "#def my_score(estimator, X, y):\n",
    "#    predict y_pred using X\n",
    "##    error_score = false_pos * 2 + false_neg \n",
    "#    if error_score > len(X) return 0\n",
    "#    else return 1 - error_score/len(X)\n",
    "\n",
    "\n",
    "# function to generate model\n",
    "def create_model(hidden_layer_size=30, dropout=(2.0/3.0), reg_penalty=0.0001):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, \n",
    "                    input_dim=869, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(reg_penalty)\n",
    "                   ))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f_score])\n",
    "    return model\n",
    "\n",
    "def selectThreshold (logits, labels, beta=1):\n",
    "    # return threshold, f-score that yields best F-score\n",
    "    # predict using true if >= threshold\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, logits)\n",
    "    f1_scores = (1 +( beta **2)) * precision * recall / ((beta ** 2) * precision + recall)\n",
    "    best_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    best_score = f1_scores[best_index]\n",
    "    return (best_threshold, best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:37:33 Starting\n",
      "\n",
      "18:37:33 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00000000, class weight: 1\n",
      "[[11345   248]\n",
      " [  106   261]]\n",
      "Xval f-score 0.636\n",
      "Xval F1 0.596\n",
      "Raw error score ----------> 0.0385\n",
      "\n",
      "18:38:40 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00000000, class weight: 2\n",
      "[[11323   209]\n",
      " [  128   300]]\n",
      "Xval f-score 0.662\n",
      "Xval F1 0.640\n",
      "Raw error score ----------> 0.0389\n",
      "\n",
      "18:39:48 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00000000, class weight: 3\n",
      "[[11371   234]\n",
      " [   80   275]]\n",
      "Xval f-score 0.683\n",
      "Xval F1 0.637\n",
      "Raw error score ----------> 0.0329\n",
      "\n",
      "18:40:55 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00001000, class weight: 1\n",
      "[[11376   220]\n",
      " [   75   289]]\n",
      "Xval f-score 0.707\n",
      "Xval F1 0.662\n",
      "Raw error score ----------> 0.0309\n",
      "\n",
      "18:42:03 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00001000, class weight: 2\n",
      "[[11379   246]\n",
      " [   72   263]]\n",
      "Xval f-score 0.677\n",
      "Xval F1 0.623\n",
      "Raw error score ----------> 0.0326\n",
      "\n",
      "18:43:13 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00001000, class weight: 3\n",
      "[[11351   216]\n",
      " [  100   293]]\n",
      "Xval f-score 0.683\n",
      "Xval F1 0.650\n",
      "Raw error score ----------> 0.0348\n",
      "\n",
      "18:44:22 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00010000, class weight: 1\n",
      "[[11338   198]\n",
      " [  113   311]]\n",
      "Xval f-score 0.691\n",
      "Xval F1 0.667\n",
      "Raw error score ----------> 0.0355\n",
      "\n",
      "18:45:32 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00010000, class weight: 2\n",
      "[[11384   241]\n",
      " [   67   268]]\n",
      "Xval f-score 0.690\n",
      "Xval F1 0.635\n",
      "Raw error score ----------> 0.0314\n",
      "\n",
      "18:46:42 Hidden layer: 10, Dropout: 0.3330, Regularization 0.00010000, class weight: 3\n",
      "[[11375   224]\n",
      " [   76   285]]\n",
      "Xval f-score 0.701\n",
      "Xval F1 0.655\n",
      "Raw error score ----------> 0.0314\n",
      "\n",
      "18:47:52 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00000000, class weight: 1\n",
      "[[11366   242]\n",
      " [   85   267]]\n",
      "Xval f-score 0.667\n",
      "Xval F1 0.620\n",
      "Raw error score ----------> 0.0344\n",
      "\n",
      "18:49:00 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00000000, class weight: 2\n",
      "[[11389   246]\n",
      " [   62   263]]\n",
      "Xval f-score 0.689\n",
      "Xval F1 0.631\n",
      "Raw error score ----------> 0.0309\n",
      "\n",
      "18:50:09 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00000000, class weight: 3\n",
      "[[11406   272]\n",
      " [   45   237]]\n",
      "Xval f-score 0.674\n",
      "Xval F1 0.599\n",
      "Raw error score ----------> 0.0303\n",
      "\n",
      "18:51:17 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00001000, class weight: 1\n",
      "[[11357   210]\n",
      " [   94   299]]\n",
      "Xval f-score 0.697\n",
      "Xval F1 0.663\n",
      "Raw error score ----------> 0.0333\n",
      "\n",
      "18:52:28 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00001000, class weight: 2\n",
      "[[11360   230]\n",
      " [   91   279]]\n",
      "Xval f-score 0.676\n",
      "Xval F1 0.635\n",
      "Raw error score ----------> 0.0344\n",
      "\n",
      "18:53:39 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00001000, class weight: 3\n",
      "[[11331   195]\n",
      " [  120   314]]\n",
      "Xval f-score 0.687\n",
      "Xval F1 0.666\n",
      "Raw error score ----------> 0.0364\n",
      "\n",
      "18:54:49 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00010000, class weight: 1\n",
      "[[11320   187]\n",
      " [  131   322]]\n",
      "Xval f-score 0.685\n",
      "Xval F1 0.669\n",
      "Raw error score ----------> 0.0375\n",
      "\n",
      "18:56:00 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00010000, class weight: 2\n",
      "[[11345   190]\n",
      " [  106   319]]\n",
      "Xval f-score 0.708\n",
      "Xval F1 0.683\n",
      "Raw error score ----------> 0.0336\n",
      "\n",
      "18:57:11 Hidden layer: 10, Dropout: 0.5000, Regularization 0.00010000, class weight: 3\n",
      "[[11357   211]\n",
      " [   94   298]]\n",
      "Xval f-score 0.696\n",
      "Xval F1 0.661\n",
      "Raw error score ----------> 0.0334\n",
      "\n",
      "18:58:22 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00000000, class weight: 1\n",
      "[[11379   261]\n",
      " [   72   248]]\n",
      "Xval f-score 0.656\n",
      "Xval F1 0.598\n",
      "Raw error score ----------> 0.0339\n",
      "\n",
      "18:59:32 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00000000, class weight: 2\n",
      "[[11397   265]\n",
      " [   54   244]]\n",
      "Xval f-score 0.672\n",
      "Xval F1 0.605\n",
      "Raw error score ----------> 0.0312\n",
      "\n",
      "19:00:42 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00000000, class weight: 3\n",
      "[[11380   256]\n",
      " [   71   253]]\n",
      "Xval f-score 0.664\n",
      "Xval F1 0.607\n",
      "Raw error score ----------> 0.0333\n",
      "\n",
      "19:01:52 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00001000, class weight: 1\n",
      "[[11393   254]\n",
      " [   58   255]]\n",
      "Xval f-score 0.683\n",
      "Xval F1 0.620\n",
      "Raw error score ----------> 0.0309\n",
      "\n",
      "19:03:03 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00001000, class weight: 2\n",
      "[[11388   248]\n",
      " [   63   261]]\n",
      "Xval f-score 0.685\n",
      "Xval F1 0.627\n",
      "Raw error score ----------> 0.0313\n",
      "\n",
      "19:04:15 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00001000, class weight: 3\n",
      "[[11340   215]\n",
      " [  111   294]]\n",
      "Xval f-score 0.673\n",
      "Xval F1 0.643\n",
      "Raw error score ----------> 0.0365\n",
      "\n",
      "19:05:27 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00010000, class weight: 1\n",
      "[[11350   210]\n",
      " [  101   299]]\n",
      "Xval f-score 0.690\n",
      "Xval F1 0.658\n",
      "Raw error score ----------> 0.0344\n",
      "\n",
      "19:06:40 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00010000, class weight: 2\n",
      "[[11343   201]\n",
      " [  108   308]]\n",
      "Xval f-score 0.693\n",
      "Xval F1 0.666\n",
      "Raw error score ----------> 0.0349\n",
      "\n",
      "19:07:53 Hidden layer: 10, Dropout: 0.6670, Regularization 0.00010000, class weight: 3\n",
      "[[11385   246]\n",
      " [   66   263]]\n",
      "Xval f-score 0.684\n",
      "Xval F1 0.628\n",
      "Raw error score ----------> 0.0316\n",
      "\n",
      "19:09:05 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00000000, class weight: 1\n",
      "[[11374   253]\n",
      " [   77   256]]\n",
      "Xval f-score 0.661\n",
      "Xval F1 0.608\n",
      "Raw error score ----------> 0.0340\n",
      "\n",
      "19:10:24 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00000000, class weight: 2\n",
      "[[11387   261]\n",
      " [   64   248]]\n",
      "Xval f-score 0.666\n",
      "Xval F1 0.604\n",
      "Raw error score ----------> 0.0325\n",
      "\n",
      "19:11:44 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00000000, class weight: 3\n",
      "[[11387   255]\n",
      " [   64   254]]\n",
      "Xval f-score 0.674\n",
      "Xval F1 0.614\n",
      "Raw error score ----------> 0.0320\n",
      "\n",
      "19:13:03 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00001000, class weight: 1\n",
      "[[11350   208]\n",
      " [  101   301]]\n",
      "Xval f-score 0.692\n",
      "Xval F1 0.661\n",
      "Raw error score ----------> 0.0343\n",
      "\n",
      "19:14:31 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00001000, class weight: 2\n",
      "[[11338   202]\n",
      " [  113   307]]\n",
      "Xval f-score 0.686\n",
      "Xval F1 0.661\n",
      "Raw error score ----------> 0.0358\n",
      "\n",
      "19:15:59 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00001000, class weight: 3\n",
      "[[11393   251]\n",
      " [   58   258]]\n",
      "Xval f-score 0.687\n",
      "Xval F1 0.625\n",
      "Raw error score ----------> 0.0307\n",
      "\n",
      "19:17:29 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00010000, class weight: 1\n",
      "[[11333   204]\n",
      " [  118   305]]\n",
      "Xval f-score 0.679\n",
      "Xval F1 0.655\n",
      "Raw error score ----------> 0.0368\n",
      "\n",
      "19:18:58 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00010000, class weight: 2\n",
      "[[11355   224]\n",
      " [   96   285]]\n",
      "Xval f-score 0.678\n",
      "Xval F1 0.640\n",
      "Raw error score ----------> 0.0348\n",
      "\n",
      "19:20:27 Hidden layer: 20, Dropout: 0.3330, Regularization 0.00010000, class weight: 3\n",
      "[[11355   222]\n",
      " [   96   287]]\n",
      "Xval f-score 0.680\n",
      "Xval F1 0.643\n",
      "Raw error score ----------> 0.0346\n",
      "\n",
      "19:21:56 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00000000, class weight: 1\n",
      "[[11376   249]\n",
      " [   75   260]]\n",
      "Xval f-score 0.669\n",
      "Xval F1 0.616\n",
      "Raw error score ----------> 0.0334\n",
      "\n",
      "19:23:16 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00000000, class weight: 2\n",
      "[[11348   225]\n",
      " [  103   284]]\n",
      "Xval f-score 0.669\n",
      "Xval F1 0.634\n",
      "Raw error score ----------> 0.0360\n",
      "\n",
      "19:24:35 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00000000, class weight: 3\n",
      "[[11358   235]\n",
      " [   93   274]]\n",
      "Xval f-score 0.667\n",
      "Xval F1 0.626\n",
      "Raw error score ----------> 0.0352\n",
      "\n",
      "19:25:55 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00001000, class weight: 1\n",
      "[[11351   214]\n",
      " [  100   295]]\n",
      "Xval f-score 0.686\n",
      "Xval F1 0.653\n",
      "Raw error score ----------> 0.0346\n",
      "\n",
      "19:27:24 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00001000, class weight: 2\n",
      "[[11368   232]\n",
      " [   83   277]]\n",
      "Xval f-score 0.683\n",
      "Xval F1 0.638\n",
      "Raw error score ----------> 0.0333\n",
      "\n",
      "19:28:53 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00001000, class weight: 3\n",
      "[[11375   228]\n",
      " [   76   281]]\n",
      "Xval f-score 0.696\n",
      "Xval F1 0.649\n",
      "Raw error score ----------> 0.0318\n",
      "\n",
      "19:30:22 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00010000, class weight: 1\n",
      "[[11345   214]\n",
      " [  106   295]]\n",
      "Xval f-score 0.679\n",
      "Xval F1 0.648\n",
      "Raw error score ----------> 0.0356\n",
      "\n",
      "19:31:54 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00010000, class weight: 2\n",
      "[[11354   221]\n",
      " [   97   288]]\n",
      "Xval f-score 0.681\n",
      "Xval F1 0.644\n",
      "Raw error score ----------> 0.0347\n",
      "\n",
      "19:33:24 Hidden layer: 20, Dropout: 0.5000, Regularization 0.00010000, class weight: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11393   252]\n",
      " [   58   257]]\n",
      "Xval f-score 0.686\n",
      "Xval F1 0.624\n",
      "Raw error score ----------> 0.0308\n",
      "\n",
      "19:34:55 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00000000, class weight: 1\n",
      "[[11348   219]\n",
      " [  103   290]]\n",
      "Xval f-score 0.676\n",
      "Xval F1 0.643\n",
      "Raw error score ----------> 0.0355\n",
      "\n",
      "19:36:16 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00000000, class weight: 2\n",
      "[[11361   227]\n",
      " [   90   282]]\n",
      "Xval f-score 0.681\n",
      "Xval F1 0.640\n",
      "Raw error score ----------> 0.0340\n",
      "\n",
      "19:37:38 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00000000, class weight: 3\n",
      "[[11385   244]\n",
      " [   66   265]]\n",
      "Xval f-score 0.687\n",
      "Xval F1 0.631\n",
      "Raw error score ----------> 0.0314\n",
      "\n",
      "19:39:00 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00001000, class weight: 1\n",
      "[[11383   233]\n",
      " [   68   276]]\n",
      "Xval f-score 0.699\n",
      "Xval F1 0.647\n",
      "Raw error score ----------> 0.0309\n",
      "\n",
      "19:40:32 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00001000, class weight: 2\n",
      "[[11367   218]\n",
      " [   84   291]]\n",
      "Xval f-score 0.699\n",
      "Xval F1 0.658\n",
      "Raw error score ----------> 0.0323\n",
      "\n",
      "19:42:04 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00001000, class weight: 3\n",
      "[[11357   215]\n",
      " [   94   294]]\n",
      "Xval f-score 0.691\n",
      "Xval F1 0.656\n",
      "Raw error score ----------> 0.0337\n",
      "\n",
      "19:43:35 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00010000, class weight: 1\n",
      "[[11353   197]\n",
      " [   98   312]]\n",
      "Xval f-score 0.708\n",
      "Xval F1 0.679\n",
      "Raw error score ----------> 0.0329\n",
      "\n",
      "19:45:08 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00010000, class weight: 2\n",
      "[[11324   177]\n",
      " [  127   332]]\n",
      "Xval f-score 0.700\n",
      "Xval F1 0.686\n",
      "Raw error score ----------> 0.0360\n",
      "\n",
      "19:46:40 Hidden layer: 20, Dropout: 0.6670, Regularization 0.00010000, class weight: 3\n",
      "[[11355   211]\n",
      " [   96   298]]\n",
      "Xval f-score 0.694\n",
      "Xval F1 0.660\n",
      "Raw error score ----------> 0.0337\n",
      "\n",
      "19:48:12 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00000000, class weight: 1\n",
      "[[11371   235]\n",
      " [   80   274]]\n",
      "Xval f-score 0.682\n",
      "Xval F1 0.635\n",
      "Raw error score ----------> 0.0330\n",
      "\n",
      "19:49:43 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00000000, class weight: 2\n",
      "[[11377   241]\n",
      " [   74   268]]\n",
      "Xval f-score 0.681\n",
      "Xval F1 0.630\n",
      "Raw error score ----------> 0.0325\n",
      "\n",
      "19:51:11 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00000000, class weight: 3\n",
      "[[11379   239]\n",
      " [   72   270]]\n",
      "Xval f-score 0.686\n",
      "Xval F1 0.635\n",
      "Raw error score ----------> 0.0320\n",
      "\n",
      "19:52:40 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00001000, class weight: 1\n",
      "[[11357   216]\n",
      " [   94   293]]\n",
      "Xval f-score 0.690\n",
      "Xval F1 0.654\n",
      "Raw error score ----------> 0.0338\n",
      "\n",
      "19:54:20 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00001000, class weight: 2\n",
      "[[11399   268]\n",
      " [   52   241]]\n",
      "Xval f-score 0.670\n",
      "Xval F1 0.601\n",
      "Raw error score ----------> 0.0311\n",
      "\n",
      "19:55:59 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00001000, class weight: 3\n",
      "[[11347   212]\n",
      " [  104   297]]\n",
      "Xval f-score 0.684\n",
      "Xval F1 0.653\n",
      "Raw error score ----------> 0.0351\n",
      "\n",
      "19:57:39 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00010000, class weight: 1\n",
      "[[11328   201]\n",
      " [  123   308]]\n",
      "Xval f-score 0.677\n",
      "Xval F1 0.655\n",
      "Raw error score ----------> 0.0374\n",
      "\n",
      "19:59:19 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00010000, class weight: 2\n",
      "[[11397   263]\n",
      " [   54   246]]\n",
      "Xval f-score 0.675\n",
      "Xval F1 0.608\n",
      "Raw error score ----------> 0.0310\n",
      "\n",
      "20:00:59 Hidden layer: 30, Dropout: 0.3330, Regularization 0.00010000, class weight: 3\n",
      "[[11378   228]\n",
      " [   73   281]]\n",
      "Xval f-score 0.700\n",
      "Xval F1 0.651\n",
      "Raw error score ----------> 0.0313\n",
      "\n",
      "20:02:39 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00000000, class weight: 1\n",
      "[[11394   265]\n",
      " [   57   244]]\n",
      "Xval f-score 0.668\n",
      "Xval F1 0.602\n",
      "Raw error score ----------> 0.0317\n",
      "\n",
      "20:04:10 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00000000, class weight: 2\n",
      "[[11378   248]\n",
      " [   73   261]]\n",
      "Xval f-score 0.673\n",
      "Xval F1 0.619\n",
      "Raw error score ----------> 0.0329\n",
      "\n",
      "20:05:41 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00000000, class weight: 3\n",
      "[[11398   261]\n",
      " [   53   248]]\n",
      "Xval f-score 0.679\n",
      "Xval F1 0.612\n",
      "Raw error score ----------> 0.0307\n",
      "\n",
      "20:07:13 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00001000, class weight: 1\n",
      "[[11384   240]\n",
      " [   67   269]]\n",
      "Xval f-score 0.691\n",
      "Xval F1 0.637\n",
      "Raw error score ----------> 0.0313\n",
      "\n",
      "20:08:53 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00001000, class weight: 2\n",
      "[[11378   239]\n",
      " [   73   270]]\n",
      "Xval f-score 0.685\n",
      "Xval F1 0.634\n",
      "Raw error score ----------> 0.0322\n",
      "\n",
      "20:10:35 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00001000, class weight: 3\n",
      "[[11360   214]\n",
      " [   91   295]]\n",
      "Xval f-score 0.696\n",
      "Xval F1 0.659\n",
      "Raw error score ----------> 0.0331\n",
      "\n",
      "20:12:18 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00010000, class weight: 1\n",
      "[[11413   270]\n",
      " [   38   239]]\n",
      "Xval f-score 0.686\n",
      "Xval F1 0.608\n",
      "Raw error score ----------> 0.0289\n",
      "\n",
      "20:14:01 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00010000, class weight: 2\n",
      "[[11379   234]\n",
      " [   72   275]]\n",
      "Xval f-score 0.693\n",
      "Xval F1 0.643\n",
      "Raw error score ----------> 0.0316\n",
      "\n",
      "20:15:42 Hidden layer: 30, Dropout: 0.5000, Regularization 0.00010000, class weight: 3\n",
      "[[11383   225]\n",
      " [   68   284]]\n",
      "Xval f-score 0.709\n",
      "Xval F1 0.660\n",
      "Raw error score ----------> 0.0302\n",
      "\n",
      "20:17:25 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00000000, class weight: 1\n",
      "[[11384   256]\n",
      " [   67   253]]\n",
      "Xval f-score 0.669\n",
      "Xval F1 0.610\n",
      "Raw error score ----------> 0.0326\n",
      "\n",
      "20:18:58 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00000000, class weight: 2\n",
      "[[11387   266]\n",
      " [   64   243]]\n",
      "Xval f-score 0.658\n",
      "Xval F1 0.596\n",
      "Raw error score ----------> 0.0329\n",
      "\n",
      "20:20:31 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00000000, class weight: 3\n",
      "[[11349   227]\n",
      " [  102   282]]\n",
      "Xval f-score 0.668\n",
      "Xval F1 0.632\n",
      "Raw error score ----------> 0.0360\n",
      "\n",
      "20:22:03 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00001000, class weight: 1\n",
      "[[11361   216]\n",
      " [   90   293]]\n",
      "Xval f-score 0.695\n",
      "Xval F1 0.657\n",
      "Raw error score ----------> 0.0331\n",
      "\n",
      "20:23:46 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00001000, class weight: 2\n",
      "[[11360   224]\n",
      " [   91   285]]\n",
      "Xval f-score 0.684\n",
      "Xval F1 0.644\n",
      "Raw error score ----------> 0.0339\n",
      "\n",
      "20:25:28 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00001000, class weight: 3\n",
      "[[11373   218]\n",
      " [   78   291]]\n",
      "Xval f-score 0.706\n",
      "Xval F1 0.663\n",
      "Raw error score ----------> 0.0313\n",
      "\n",
      "20:27:12 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00010000, class weight: 1\n",
      "[[11380   230]\n",
      " [   71   279]]\n",
      "Xval f-score 0.699\n",
      "Xval F1 0.650\n",
      "Raw error score ----------> 0.0311\n",
      "\n",
      "20:28:56 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00010000, class weight: 2\n",
      "[[11354   216]\n",
      " [   97   293]]\n",
      "Xval f-score 0.687\n",
      "Xval F1 0.652\n",
      "Raw error score ----------> 0.0343\n",
      "\n",
      "20:30:40 Hidden layer: 30, Dropout: 0.6670, Regularization 0.00010000, class weight: 3\n",
      "[[11391   243]\n",
      " [   60   266]]\n",
      "Xval f-score 0.696\n",
      "Xval F1 0.637\n",
      "Raw error score ----------> 0.0304\n",
      "20:32:23 Finishing\n"
     ]
    }
   ],
   "source": [
    "print('%s Starting' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# minimize a raw score\n",
    "# show correct metric\n",
    "# pick threshold using correct metric\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=100, batch_size=100, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "hidden_layer_hp = [10, 20, 30]\n",
    "dropout_hp = [0.333, 0.5, 0.667]\n",
    "reg_penalty_hp = [0.00001, 0.0001, 0.001]\n",
    "class_weight1= {0: 1.0, 1: 1.0}\n",
    "class_weight2= {0: 1.0, 1: 2.0}\n",
    "class_weight3 = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight_hp = [class_weight1, class_weight2, class_weight3]\n",
    "\n",
    "param_grid = dict(hidden_layer_size=hidden_layer_hp, \n",
    "                  dropout=dropout_hp, \n",
    "                  reg_penalty=reg_penalty_hp,\n",
    "                  class_weight=class_weight_hp)\n",
    "\n",
    "#grid = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=kfold, n_jobs=-1)\n",
    "#classifier = grid.fit(X_train, y_train)\n",
    "\n",
    "#GridSearchCV is annoying to use with custom metric to choose best, sted accuracy\n",
    "#would like to do k-fold CV and average raw error but not clear how to do that\n",
    "#also would like output after each iteration\n",
    "\n",
    "for hl in hidden_layer_hp:\n",
    "    for dr in dropout_hp:\n",
    "        for rp in reg_penalty_hp:\n",
    "            count=0\n",
    "            for cw in class_weight_hp:\n",
    "                count+=1\n",
    "                print(\"\\n%s Hidden layer: %d, Dropout: %.4f, Regularization %.8f, class weight: %d\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"), hl, dr, rp, count))               \n",
    "                classifier = KerasClassifier(build_fn=create_model, epochs=100, batch_size=100, verbose=0,\n",
    "#                                        cv=kfold,\n",
    "                                        hidden_layer_size=hl, \n",
    "                                        dropout=dr, \n",
    "                                        reg_penalty=rp,\n",
    "                                        class_weight=cw\n",
    "                                       )\n",
    "                classifier.fit(X_train, y_train)\n",
    "                                \n",
    "                y_xval_proba = classifier.predict_proba(X_xval)[:,1]\n",
    "                thresh, score = selectThreshold(y_xval_proba, y_xval, beta=(2.0/3))\n",
    "                y_xval_pred = y_xval_proba >= thresh\n",
    "                \n",
    "                confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "                print(confusion_matrix)\n",
    "                true_negatives = confusion_matrix[0][0]\n",
    "                false_negatives = confusion_matrix[0][1]\n",
    "                false_positives = confusion_matrix[1][0]\n",
    "                true_positives = confusion_matrix[1][1]\n",
    "                total_observations = len(y_xval_pred)\n",
    "                \n",
    "                print(\"Xval f-score %.3f\" % score)\n",
    "                print(\"Xval F1 %.3f\" % sklearn.metrics.f1_score(y_xval_pred, y_xval))\n",
    "                print(\"Raw error score ----------> %.4f\" % ((false_positives*2 + false_negatives) / float(total_observations)))                \n",
    "\n",
    "print('%s Finishing' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (classifier.best_score_, classifier.best_params_))\n",
    "# means = classifier.cv_results_['mean_test_score']\n",
    "# stds = classifier.cv_results_['std_test_score']\n",
    "# params = classifier.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "#kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "#print(\"\\nResults: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11385   232]\n",
      " [   66   277]]\n",
      "Xval f-score 0.703\n",
      "Xval F1 0.650\n",
      "Raw error score ----------> 0.0304\n",
      "[[11412   214]\n",
      " [   86   248]]\n",
      "Test f-score 0.664\n",
      "Test F1 0.623\n",
      "Raw error score ----------> 0.0323\n"
     ]
    }
   ],
   "source": [
    "# select preferred parameters using raw error and f-score\n",
    "\n",
    "hl = 20\n",
    "dr = 0.6670\n",
    "rp = 0.00010000\n",
    "cw = {0: 1.0, 1: 1.0}\n",
    "\n",
    "classifier = KerasClassifier(build_fn=create_model, epochs=300, batch_size=500, verbose=0,\n",
    "                             hidden_layer_size=hl, \n",
    "                             dropout=dr, \n",
    "                             reg_penalty=rp,\n",
    "                             class_weight=cw\n",
    "                             )\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_xval_proba = classifier.predict_proba(X_xval)[:,1]\n",
    "thresh, score = selectThreshold(y_xval_proba, y_xval, beta=(2.0/3))\n",
    "y_xval_pred = y_xval_proba >= thresh\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "print(confusion_matrix)\n",
    "true_negatives = confusion_matrix[0][0]\n",
    "false_negatives = confusion_matrix[0][1]\n",
    "false_positives = confusion_matrix[1][0]\n",
    "true_positives = confusion_matrix[1][1]\n",
    "total_observations = len(y_xval_pred)\n",
    "                \n",
    "print(\"Xval f-score %.3f\" % score)\n",
    "print(\"Xval F1 %.3f\" % sklearn.metrics.f1_score(y_xval_pred, y_xval))\n",
    "print(\"Raw error score ----------> %.4f\" % ((false_positives*2 + false_negatives) / float(total_observations)))                \n",
    "\n",
    "y_test_proba = classifier.predict_proba(X_test)[:,1]\n",
    "thresh, score = selectThreshold(y_test_proba, y_test, beta=(2.0/3))\n",
    "y_test_pred = y_test_proba >= thresh\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test_pred, y_test)\n",
    "print(confusion_matrix)\n",
    "true_negatives = confusion_matrix[0][0]\n",
    "false_negatives = confusion_matrix[0][1]\n",
    "false_positives = confusion_matrix[1][0]\n",
    "true_positives = confusion_matrix[1][1]\n",
    "total_observations = len(y_test_pred)\n",
    "\n",
    "print(\"Test f-score %.3f\" % score)\n",
    "print(\"Test F1 %.3f\" % sklearn.metrics.f1_score(y_test_pred, y_test))\n",
    "print(\"Raw error score ----------> %.4f\" % ((false_positives*2 + false_negatives) / float(total_observations)))                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
